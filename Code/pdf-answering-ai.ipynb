{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read PDF\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# get text or context from your pdf\n",
    "pdf_path = 'your pdf path'  #enter your pdf path here \n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:03:24.533679Z",
     "iopub.status.busy": "2024-06-19T18:03:24.532412Z",
     "iopub.status.idle": "2024-06-19T18:03:30.817682Z",
     "shell.execute_reply": "2024-06-19T18:03:30.816542Z",
     "shell.execute_reply.started": "2024-06-19T18:03:24.533640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset for fine tuning of the model\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# divide the dataset into training and validation\n",
    "small_dataset = dataset[\"train\"]\n",
    "small_eval_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:03:39.406887Z",
     "iopub.status.busy": "2024-06-19T18:03:39.405967Z",
     "iopub.status.idle": "2024-06-19T18:03:42.807446Z",
     "shell.execute_reply": "2024-06-19T18:03:42.806523Z",
     "shell.execute_reply.started": "2024-06-19T18:03:39.406851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:03:47.535259Z",
     "iopub.status.busy": "2024-06-19T18:03:47.534873Z",
     "iopub.status.idle": "2024-06-19T18:03:47.545782Z",
     "shell.execute_reply": "2024-06-19T18:03:47.544419Z",
     "shell.execute_reply.started": "2024-06-19T18:03:47.535228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define preprocess function\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "        if not (offset[context_start][0] <= start_char and offset[context_end][1] >= end_char):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_position = context_start\n",
    "            while start_position < len(offset) and offset[start_position][0] <= start_char:\n",
    "                start_position += 1\n",
    "            start_positions.append(start_position - 1)\n",
    "\n",
    "            end_position = context_end\n",
    "            while end_position >= 0 and offset[end_position][1] >= end_char:\n",
    "                end_position -= 1\n",
    "            end_positions.append(end_position + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:03:55.882927Z",
     "iopub.status.busy": "2024-06-19T18:03:55.882542Z",
     "iopub.status.idle": "2024-06-19T18:03:56.920001Z",
     "shell.execute_reply": "2024-06-19T18:03:56.919096Z",
     "shell.execute_reply.started": "2024-06-19T18:03:55.882895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "tokenized_datasets = small_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_datasets = small_eval_dataset.map(preprocess_function, batched =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:04:05.669167Z",
     "iopub.status.busy": "2024-06-19T18:04:05.668521Z",
     "iopub.status.idle": "2024-06-19T18:04:05.673510Z",
     "shell.execute_reply": "2024-06-19T18:04:05.672417Z",
     "shell.execute_reply.started": "2024-06-19T18:04:05.669136Z"
    }
   },
   "outputs": [],
   "source": [
    "# define training and evaluation dataset\n",
    "train_dataset = tokenized_datasets\n",
    "eval_dataset = tokenized_eval_datasets  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T18:04:07.355043Z",
     "iopub.status.busy": "2024-06-19T18:04:07.354224Z",
     "iopub.status.idle": "2024-06-19T18:43:31.120197Z",
     "shell.execute_reply": "2024-06-19T18:43:31.118927Z",
     "shell.execute_reply.started": "2024-06-19T18:04:07.355010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer with both train and eval datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model and tokenizer\n",
    "trained_model = trainer.model\n",
    "trained_tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload your question here\n",
    "question = \"What is the title of this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the answer from the model\n",
    "def get_answer_from_model(trained_model, trained_tokenizer, question, context):\n",
    "    # Tokenize the question and context\n",
    "    inputs = trained_tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Get the model's outputs\n",
    "    outputs = trained_model(**inputs)\n",
    "\n",
    "    # Get the most likely beginning and end of the answer span\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "    # Convert token indices to text\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    answer = trained_tokenizer.convert_tokens_to_string(trained_tokenizer.convert_ids_to_tokens(input_ids[answer_start_index:answer_end_index]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answer from the model\n",
    "answer = get_answer_from_model(trained_model, trained_tokenizer, question, pdf_text)\n",
    "\n",
    "\n",
    "# print the question and answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
